% ============================================================================
% APPROACH
% ============================================================================
\section{Approach}
\label{sec:approach}

This section describes our predictive modeling methodology, followed by the experimental platform used to generate recovery scenarios.

\subsection{Predictive Modeling Methodology}
\label{sec:ml-methodology}

We develop predictive models for three workflow recovery outcomes: recovery time, recovery success, and state completeness.

\subsubsection{Dataset Generation}

We generate synthetic recovery scenarios by systematically varying parameters:
\begin{itemize}
    \item \textbf{Checkpoint count}: 1--200 (log-uniform distribution)
    \item \textbf{Corruption level}: 0\%, 10\%, 30\%, 50\%, 70\%, 90\%
    \item \textbf{Interruption type}: graceful, abrupt, crash, timeout
    \item \textbf{Project type}: research, web, ml, devops, data, mobile, fullstack
\end{itemize}

We generate 1,000 unique parameter combinations and execute 3 independent trials per combination, yielding 3,000 annotated scenarios. Each trial runs the actual \texttt{recover\_context.sh} script and records ground-truth outcomes.

\subsubsection{Feature Engineering}

We extract 18 features from each recovery scenario (Table~\ref{tab:features}):

\begin{table}[t]
\caption{Feature Descriptions (18 total: 13 numerical, 5 categorical)}
\label{tab:features}
\small
\begin{tabular}{llp{4.5cm}}
\toprule
\textbf{Feature} & \textbf{Type} & \textbf{Description} \\
\midrule
checkpoint\_count & Num. & Number of checkpoints in log \\
corruption\_level & Num. & Fraction of bytes corrupted (0--1) \\
handoff\_chars & Num. & Character count in handoff.md \\
state\_yaml\_size & Num. & Size of state.yaml in bytes \\
phase\_progress & Num. & Progress percentage (0--100) \\
interruption\_type & Cat. & Type of simulated interruption \\
project\_type & Cat. & Category of workflow \\
agent\_state & Cat. & Active agent when interrupted \\
\bottomrule
\end{tabular}
\end{table}

Categorical features (5 total: project\_type, agent\_state, interruption\_type, state\_complexity, handoff\_size) are encoded using scikit-learn's LabelEncoder~\cite{pedregosa2011scikit}.

\subsubsection{Data Preprocessing}

We apply StandardScaler to all numerical features before model training, transforming each feature to zero mean and unit variance. Missing values (rare, $<$0.1\% of data) are imputed with zeros using \texttt{np.nan\_to\_num}. We use stratified sampling for train-test split (80\%/20\%) to preserve class distribution in the binary classification task.

\subsubsection{Model Selection}

We evaluate four model families:
\begin{enumerate}
    \item \textbf{Linear Regression / Logistic Regression}: Baseline assuming linear relationships between features and targets.
    \item \textbf{Ridge Regression}: L2-regularized linear model ($\alpha=1.0$).
    \item \textbf{Random Forest}: Ensemble of 100 decision trees with bagging.
    \item \textbf{Gradient Boosting}: Sequential ensemble of 100 weak learners.
\end{enumerate}

We use scikit-learn~\cite{pedregosa2011scikit} implementations with default hyperparameters (n\_estimators=100, random\_state=42) for reproducibility. While hyperparameter tuning could improve results marginally, our goal is to establish baseline predictive performance rather than optimize for competition accuracy.

\subsubsection{Target Variables}

We predict three outcomes:
\begin{enumerate}
    \item \textbf{Recovery Time} (continuous): Milliseconds to complete recovery script execution.
    \item \textbf{Recovery Success} (binary): 1 if state\_completeness $>$ 50\%, else 0.
    \item \textbf{State Completeness} (continuous): Percentage of original state recovered (0--100\%).
\end{enumerate}

\subsubsection{Evaluation Metrics}

For regression tasks: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and coefficient of determination ($R^2$).

For classification: Area Under ROC Curve (AUC-ROC), F1 score, and accuracy.

We report 95\% confidence intervals computed via t-distribution from 5-fold cross-validation results.

\subsubsection{Baseline Comparisons}

To demonstrate that ML models learn non-trivial patterns, we compare against:
\begin{itemize}
    \item \textbf{Random Baseline}: Predict mean (regression) or majority class (classification)
    \item \textbf{Single-Feature Heuristic}: Predict failure if corruption\_level $>$ 0.5
\end{itemize}

% ============================================================================
% EXPERIMENTAL PLATFORM (Compressed UWS Description)
% ============================================================================
\subsection{Experimental Platform: UWS}
\label{sec:uws}

We use the Universal Workflow System (UWS) as our experimental platform for generating recovery scenarios. UWS provides:

\begin{enumerate}
    \item \textbf{Git-native state persistence}: All workflow state stored in version-controlled YAML files, enabling checkpoint-based recovery.
    \item \textbf{Structured checkpoints}: Each checkpoint captures project phase, agent state, and human-readable handoff notes.
    \item \textbf{Fallback mechanism}: When primary state is corrupted, UWS attempts recovery from handoff documents (human-readable Markdown).
\end{enumerate}

\paragraph{State Files} The \texttt{.workflow/} directory contains:
\begin{itemize}
    \item \texttt{state.yaml}: Current phase, checkpoint ID, metadata
    \item \texttt{checkpoints.log}: Timestamped checkpoint history
    \item \texttt{handoff.md}: Human-readable context for session continuity
\end{itemize}

\paragraph{Checkpoint Mechanism} Creating a checkpoint involves: (1) updating \texttt{state.yaml} with current context, (2) appending to \texttt{checkpoints.log} with timestamp and description, and (3) optionally committing changes via git.

\paragraph{Recovery Process} Context recovery reads checkpoint files and restores state by loading \texttt{state.yaml}, parsing \texttt{checkpoints.log} for recent history, and extracting context from \texttt{handoff.md}.

\paragraph{Implementation} UWS is implemented in Bash (2,000 LOC) with no external dependencies beyond git and standard Unix utilities. The full implementation, including 356 tests (93\% pass rate for core functionality, 76\% for experimental features), is available in our replication package.

\paragraph{Why UWS as Platform} We chose UWS because: (1) its file-based state enables controlled corruption experiments, (2) its checkpoint mechanism provides ground-truth recovery outcomes, and (3) its open-source implementation enables full reproducibility.

\paragraph{Corruption Simulation} To generate training data, we simulate corruption by:
\begin{enumerate}
    \item Creating valid checkpoint state
    \item Applying byte-level random corruption at specified levels (0--90\%)
    \item Executing recovery and measuring outcomes
\end{enumerate}

This controlled methodology enables causal analysis of factors affecting recovery, which is impossible with observational data from production systems.

