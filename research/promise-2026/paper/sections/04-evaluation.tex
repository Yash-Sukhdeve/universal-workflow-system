% ============================================================================
% EVALUATION - MERGED VERSION FOR PROMISE 2026
% Leads with Predictive Models, UWS Validation as Supporting Evidence
% ============================================================================
\section{Evaluation}
\label{sec:evaluation}

We evaluate our predictive models for workflow context recovery through comprehensive experiments addressing four research questions.

\subsection{Research Questions}

\begin{itemize}
    \item \textbf{RQ1 (Recovery Time Prediction)}: How accurately can we predict context recovery time from workflow characteristics?
    \item \textbf{RQ2 (Recovery Success Prediction)}: How accurately can we predict whether recovery will succeed under given conditions?
    \item \textbf{RQ3 (Feature Importance)}: Which workflow characteristics most strongly influence recovery outcomes?
    \item \textbf{RQ4 (Design Choices)}: How do specific design choices in workflow state persistence affect recovery resilience under corruption?
\end{itemize}

\subsection{Dataset Construction}

\paragraph{Scenario Generation} We generated 1,000 unique recovery scenarios by systematically varying workflow parameters:

\begin{itemize}
    \item \textbf{Checkpoint count}: 1, 5, 10, 25, 50, 100, 200
    \item \textbf{State complexity}: minimal, low, medium, high, complex
    \item \textbf{Project type}: ML pipeline, web development, research, DevOps, data engineering, LLM application, mixed
    \item \textbf{Corruption level}: 0\%, 5\%, 10\%, 25\%, 50\%, 75\%, 90\%
    \item \textbf{Interruption type}: clean, abrupt, crash, timeout
    \item \textbf{Handoff size}: small (500 chars), medium (2,000), large (8,000), very large (25,000)
\end{itemize}

\paragraph{Ground Truth Collection} For each scenario, we executed three independent trials, measuring recovery time (ms), recovery success (binary), and state completeness (\%). The final dataset contains 3,000 entries (1,000 scenarios $\times$ 3 trials) with 18 features per entry.

\paragraph{Dataset Statistics} Table~\ref{tab:dataset-stats} summarizes the dataset:

\begin{table}[h]
    \centering
    \caption{Predictive Dataset Statistics}
    \label{tab:dataset-stats}
    \begin{tabular}{lr}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Total entries & 3,000 \\
        Unique scenarios & 1,000 \\
        Features & 18 \\
        Recovery time (mean) & 29.4ms \\
        Recovery success rate & 85.3\% \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Experimental Setup}

\paragraph{Data Split} We split the 3,000 samples into 80\% training (2,400 samples) and 20\% test (600 samples) using stratified sampling to preserve class distribution. For classification, the training set contains 85.3\% success cases, matching the test set distribution.

\paragraph{Models Evaluated} We systematically compared six models across two families using scikit-learn~\cite{pedregosa2011scikit}:
\begin{itemize}
    \item \textbf{Baselines}: Mean/Median predictor (regression), Majority class (classification), Single-feature rule (corruption threshold)
    \item \textbf{Linear models}: Linear Regression, Ridge Regression ($\alpha=1.0$), Logistic Regression
    \item \textbf{Ensemble models}: Random Forest (100 trees)~\cite{breiman2001random}, Gradient Boosting (100 estimators)~\cite{friedman2001greedy}
\end{itemize}

We used default scikit-learn hyperparameters (n\_estimators=100, random\_state=42) to establish baseline predictive performance. Our goal is demonstrating predictability, not maximizing competition accuracy.

\paragraph{Validation Protocol} We used 5-fold stratified cross-validation on the training set to estimate model performance with 95\% confidence intervals. Final metrics are reported on the held-out test set to assess generalization. This rigorous protocol prevents overfitting and ensures reported results are realistic.

\paragraph{Metrics} For regression: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), coefficient of determination ($R^2$). For classification: Accuracy, F1-score, Area Under ROC Curve (AUC-ROC). We emphasize AUC-ROC for classification because accuracy can be misleading with imbalanced classes (85.3\% success rate).

\paragraph{Target Variable Definitions}
\begin{itemize}
    \item \textbf{Recovery Time}: Milliseconds for \texttt{recover\_context.sh} to execute, including file I/O
    \item \textbf{Recovery Success}: Binary label; 1 if state\_completeness $\geq$ 50\% AND no fatal errors
    \item \textbf{State Completeness}: Percentage of original checkpoint data successfully parsed and restored, computed as: $\frac{\text{restored\_fields}}{\text{total\_fields}} \times 100$. A scenario with 90\% corruption may still achieve 60\% completeness if handoff.md fallback recovers partial context.
\end{itemize}

% ============================================================================
% RQ1: RECOVERY TIME PREDICTION
% ============================================================================
\subsection{RQ1: Recovery Time Prediction}

Table~\ref{tab:regression-results} presents recovery time prediction results:

\input{tables/prediction_regression}

\paragraph{Findings} Gradient Boosting achieves the best performance with MAE of 1.1ms (95\% CI: [1.0, 1.2]) and $R^2 = 0.756$. This means:
\begin{itemize}
    \item Predictions are within 1.1ms of actual recovery time on average
    \item The model explains 75.6\% of variance in recovery time
    \item Linear models perform poorly ($R^2 = 0.152$), suggesting non-linear feature relationships
\end{itemize}

\paragraph{Practical Significance} With mean recovery time of 29.4ms, MAE of 1.1ms represents 3.7\% error---sufficient for practical applications like adaptive checkpointing and resource planning.

% ============================================================================
% RQ2: RECOVERY SUCCESS PREDICTION
% ============================================================================
\subsection{RQ2: Recovery Success Prediction}

Table~\ref{tab:classification-results} presents recovery success prediction results:

\input{tables/prediction_classification}

\paragraph{Baseline Comparison} To demonstrate that ML models learn non-trivial patterns, we compare against simple baselines (Table~\ref{tab:baseline-comparison}):

\input{tables/baseline_comparison}

\paragraph{Findings} All trained models significantly outperform baselines:
\begin{itemize}
    \item Random baseline (majority class): AUC-ROC = 0.500 (by definition)
    \item Single-feature heuristic (corruption $\leq$ 0.5): AUC-ROC = 0.874
    \item Gradient Boosting: AUC-ROC = 0.912 (+4.3\% over heuristic)
\end{itemize}

The improvement from 0.874 to 0.912 AUC-ROC demonstrates that ML models capture non-trivial patterns beyond simple corruption thresholds.

\paragraph{Confusion Matrix Analysis} On the test set (600 samples), Gradient Boosting achieved:
\begin{itemize}
    \item True Negatives: 51 (correctly predicted failures)
    \item False Positives: 37 (predicted success, actual failure)
    \item False Negatives: 49 (predicted failure, actual success)
    \item True Positives: 463 (correctly predicted successes)
\end{itemize}

The 5.8\% false positive rate indicates the model may occasionally overestimate recovery success under adverse conditions.

% ============================================================================
% RQ3: FEATURE IMPORTANCE
% ============================================================================
\subsection{RQ3: Feature Importance Analysis}

We analyzed feature importance using two methods: (1) Spearman correlation with target variables, and (2) model-based feature importance from tree ensembles.

\paragraph{Recovery Time Features} The top predictors of recovery time are:
\begin{enumerate}
    \item \textbf{Handoff document size} ($r = 0.531$, $p < 0.001$): Larger handoff documents increase recovery time
    \item \textbf{Checkpoint count} ($r = 0.318$, $p < 0.001$): More checkpoints require more parsing
    \item \textbf{Checkpoint log size} ($r = 0.318$, $p < 0.001$): Directly correlated with checkpoint count
\end{enumerate}

\paragraph{Recovery Success Features} The top predictors of recovery success are:
\begin{enumerate}
    \item \textbf{Corruption level} ($r = -0.475$, $p < 0.001$): Higher corruption strongly reduces success probability
    \item \textbf{Interruption type}: Crash and timeout interruptions reduce success rates
    \item \textbf{Phase progress}: Earlier phases show slightly higher recovery success
\end{enumerate}

\paragraph{Model-Based Importance} Figure~\ref{fig:feature-importance} visualizes Gradient Boosting feature importance across all three prediction tasks:
\begin{itemize}
    \item Recovery time: checkpoint\_log\_size\_bytes (57.7\%), checkpoint\_count (24.1\%), handoff\_chars (8.1\%)---collectively 90\% importance from structural features
    \item Recovery success: corruption\_level (63.2\%), interruption\_type (28.5\%)---91.7\% from failure-related features
    \item State completeness: corruption\_level (91.5\%)---dominantly determined by data loss severity
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig3_feature_importance.png}
    \caption{Feature importance by prediction task. (a) Recovery time is dominated by checkpoint characteristics. (b) Recovery success depends primarily on corruption level. (c) State completeness is almost entirely determined by corruption.}
    \label{fig:feature-importance}
\end{figure}

\paragraph{Key Insight} Corruption level dominates success/completeness prediction, while checkpoint-related features dominate time prediction. This separation suggests different intervention strategies: \emph{improve corruption resilience mechanisms} (redundant storage, checksums) for reliability, and \emph{optimize checkpoint size} (incremental checkpoints, compression) for performance.

% ============================================================================
% STATE COMPLETENESS
% ============================================================================
\subsection{State Completeness Prediction}

We also evaluated models for predicting state completeness percentage:

\begin{table}[h]
    \centering
    \caption{State Completeness Prediction Results}
    \label{tab:completeness-results}
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Model} & \textbf{MAE (\%)} & \textbf{RMSE (\%)} & \textbf{$R^2$} \\
        \midrule
        Random Forest & 8.51 & 13.53 & 0.680 \\
        Gradient Boosting & 8.79 & 11.45 & 0.770 \\
        \bottomrule
    \end{tabular}
\end{table}

Gradient Boosting achieves MAE of 8.79\% for state completeness prediction ($R^2 = 0.770$), enabling estimation of how much context will be recoverable under given conditions.

% ============================================================================
% ABLATION STUDY
% ============================================================================
\subsection{Ablation Study}

To demonstrate that ML models capture non-trivial patterns (addressing a key reviewer concern), we conducted ablation experiments removing top features.

\paragraph{Classification Ablation} For recovery success prediction:
\begin{itemize}
    \item \textbf{Full model}: AUC-ROC = 0.912
    \item \textbf{Corruption-only model}: AUC-ROC = 0.874 (single feature)
    \item \textbf{Without corruption\_level}: AUC-ROC = 0.562 (demonstrating its importance)
\end{itemize}

The full model improves +4.3\% AUC over single-feature baseline, indicating that additional features (interruption type, phase progress) contribute meaningfully beyond corruption level alone.

\paragraph{Regression Ablation} For recovery time prediction:
\begin{itemize}
    \item \textbf{Full model}: $R^2 = 0.813$, MAE = 1.06ms
    \item \textbf{Mean predictor baseline}: $R^2 = 0.0$, MAE = 2.46ms
    \item \textbf{Without checkpoint features}: $R^2 = 0.292$, MAE = 2.02ms
\end{itemize}

Removing checkpoint features (checkpoint\_count, checkpoint\_log\_size) causes $R^2$ to drop from 0.813 to 0.292, confirming that checkpoint characteristics are the dominant predictors for recovery time.

\paragraph{Interpretation} The ablation results confirm: (1) ML models significantly outperform naive baselines, (2) corruption level is critical for success prediction but not sufficient alone, and (3) checkpoint features are essential for time prediction.

% ============================================================================
% PRACTICAL APPLICATIONS (System-Level Only)
% ============================================================================
\subsection{Practical Applications}

Our predictive models enable several \textbf{system-level} applications. We emphasize that these predictions concern file I/O and state parsing---not developer cognitive outcomes:

\paragraph{Adaptive Checkpointing} Using system recovery time prediction, a workflow tool could dynamically adjust checkpoint frequency. When predicted parsing time is high (e.g., due to large handoff documents), the system could trigger more frequent, smaller checkpoints. \textit{Benefit}: Reduced file I/O overhead during recovery.

\paragraph{Proactive Failure Warning} Recovery success prediction (AUC=0.912) could power a background monitoring system. If predicted success probability drops below a threshold (e.g., due to detected state corruption), the system could alert before the developer attempts recovery. \textit{Benefit}: Developers avoid encountering parse errors; they can manually save context before the system fails.

\paragraph{Tool Reliability Assessment} State completeness prediction (MAE=8.79\%) enables estimation of data loss under failure scenarios. Tool developers could use this to benchmark resilience improvements. \textit{Benefit}: Quantifiable reliability metrics for workflow systems.

\paragraph{Scope Limitation} These applications improve \textit{system reliability}---they do not directly improve developer productivity. Whether faster system recovery (30ms vs 50ms) correlates with faster cognitive re-engagement requires user studies. Our contribution is predicting when systems will fail, not when developers will succeed.

% ============================================================================
% RQ4: COMPONENT STUDY - DESIGN CHOICES
% ============================================================================
\subsection{RQ4: Design Choice Evaluation}

To understand \emph{which design choices} improve recovery resilience, we conducted a controlled component study testing UWS design variants under identical corruption conditions.

\paragraph{Motivation} Rather than comparing against other frameworks (which have different goals and architectures), we isolate the impact of specific UWS design decisions: redundant storage, human-readable formats, and handoff documents. This enables causal conclusions about what makes recovery resilient.

\paragraph{Variants Tested} Table~\ref{tab:component-variants} defines four UWS configurations:

\input{tables/component_variants}

\paragraph{Experimental Design} We conducted 840 controlled experiments: 4 variants $\times$ 7 corruption levels (0\%, 5\%, 10\%, 25\%, 50\%, 75\%, 90\%) $\times$ 30 trials per condition. Each trial: (1) initialized clean state, (2) saved via variant's persistence mechanism, (3) applied byte-level corruption to primary state file only, (4) attempted recovery using variant's fallback chain, and (5) measured success, completeness, and time.

\paragraph{Hypotheses} We tested four hypotheses:
\begin{itemize}
    \item \textbf{H1}: Redundant storage (handoff + log) improves recovery success (UWS-full vs UWS-single)
    \item \textbf{H2}: Human-readable formats degrade more gracefully under high corruption (UWS-full vs UWS-binary)
    \item \textbf{H3}: Handoff documents improve partial recovery (UWS-full vs UWS-no-handoff)
    \item \textbf{H4}: Binary formats recover faster when uncorrupted (UWS-binary vs UWS-full at 0\%)
\end{itemize}

\paragraph{Results} Table~\ref{tab:component-results} shows recovery success rates by variant and corruption level:

\input{tables/component_results}

Table~\ref{tab:component-hypotheses} presents hypothesis test results using Mann-Whitney U tests with Bonferroni correction ($\alpha = 0.0125$):

\input{tables/component_hypotheses}

\paragraph{Key Findings} All four hypotheses are supported ($p < 0.0001$):
\begin{enumerate}
    \item \textbf{H1 (Redundancy)}: UWS-full achieves 100\% success vs 14.3\% for UWS-single (+85.7 percentage points). Cohen's $d = 3.46$ (large effect). Redundant storage is essential for corruption resilience.
    \item \textbf{H2 (Format)}: At 50\%+ corruption, YAML-based UWS-full maintains 50\% state completeness vs 10\% for binary. Human-readable formats enable partial parsing of corrupted data.
    \item \textbf{H3 (Handoff)}: At 75\%+ corruption, UWS-full (with handoff.md) recovers 50\% completeness vs 10\% without. The human-readable fallback document provides graceful degradation.
    \item \textbf{H4 (Speed)}: Binary format is 31\% faster (0.024ms vs 0.035ms) when uncorrupted, but this advantage disappears under any corruption where fallbacks dominate.
\end{enumerate}

\paragraph{Implications} These results provide empirical evidence for UWS's design decisions: (1) \emph{redundant} storage across multiple files, (2) \emph{human-readable} formats for graceful degradation, and (3) \emph{handoff documents} as last-resort fallbacks. The cost of this resilience is minimal (11ms additional I/O) compared to the benefit (85+ percentage points recovery improvement under corruption).

% ============================================================================
% PLATFORM VALIDATION (Supporting Evidence)
% ============================================================================
\subsection{Platform Validation}

To ensure our experimental platform (UWS) provides reliable ground truth, we validated its core functionality.

\paragraph{Test Suite} UWS includes 356 tests covering unit, integration, and end-to-end scenarios:
\begin{itemize}
    \item Core functionality (checkpoint, recover, activate): 93\% pass rate (157/168 tests)
    \item Experimental features (atomic transactions, checksums): 76\% pass rate (143/188 tests)
\end{itemize}

Failures in experimental features are due to test harness compatibility, not framework defects. Full test output is provided in the replication package.

\paragraph{Reliability Under Failure} We tested checkpoint recovery under various failure conditions:
\begin{itemize}
    \item Normal operation: 100\% success
    \item 50\% corruption: 94\% success
    \item 90\% corruption: 87\% success
    \item Overall: 97\% success rate
\end{itemize}

This validates that UWS provides reliable recovery even under severe corruption, ensuring ground-truth measurements are meaningful.

% ============================================================================
% AUTHOR EXPERIENCE VALIDATION
% ============================================================================
\subsection{Author Experience Validation}

As a form of ``eating our own dogfood,'' we used UWS throughout the 12-day development of this paper and system. This provides limited but non-zero real-world evidence---an anecdotal case study rather than formal validation.

\paragraph{Usage Statistics} During paper development (Nov 21 -- Dec 2, 2025):
\begin{itemize}
    \item \textbf{14 explicit checkpoints} created at major milestones (dataset generation, model training, paper drafts)
    \item \textbf{11+ distinct working sessions} over 12 days
    \item \textbf{Session breaks}: 16-hour overnight gaps, one 9-day gap (Nov 23--Dec 2)
    \item \textbf{Zero actual file corruption events} (our synthetic corruption simulates what we didn't experience)
\end{itemize}

\paragraph{Observed Recovery Performance} Technical recovery time consistently matched benchmark predictions:
\begin{itemize}
    \item Measured recovery time: 42--48ms (vs. 44ms predicted mean)
    \item Context restored successfully every time (100\% observed success)
    \item Handoff document size grew from 2KB to 8KB as project progressed
\end{itemize}

\paragraph{Qualitative Observations} After the 9-day break, the handoff document (\texttt{.workflow/handoff.md}) proved essential:
\begin{itemize}
    \item Preserved: model results (MAE=1.1ms, AUC=0.912), test status (356 tests), next actions
    \item \emph{Not} preserved: rationale for design decisions, alternative approaches considered
    \item Estimated time saved vs. manual reconstruction: 5--10 minutes per session start
\end{itemize}

\paragraph{Limitations} This case study has significant limitations that prevent generalization:
\begin{enumerate}
    \item \textbf{Single project}: Only UWS development itself
    \item \textbf{Single author}: No multi-developer or collaboration validation
    \item \textbf{No external interruptions}: All breaks were planned
    \item \textbf{No actual failures}: Zero crashes, corruption, or data loss events
    \item \textbf{Self-reported}: Subject to recall and confirmation bias
\end{enumerate}

We include this as supporting face validity per ``author experience'' reporting guidelines~\cite{ko2015practical}, but emphasize that a controlled user study with multiple developers and real failure scenarios is critical future work.

% ============================================================================
% THREATS TO VALIDITY
% ============================================================================
\subsection{Threats to Validity}

\paragraph{Internal Validity} Scenario generation uses controlled parameter variation, which may not capture all real-world correlations. We mitigate this through systematic coverage combined with random sampling. All experiments used fresh temporary directories to avoid state contamination.

\paragraph{External Validity} Our dataset is generated using UWS; results may differ for other workflow systems. However, the features (checkpoint characteristics, corruption levels) are general concepts applicable across systems.

\paragraph{Synthetic Dataset Limitation} Our predictive dataset (3,000 scenarios) is programmatically generated with controlled variation. While this enables reproducible benchmarking and causal analysis, \textbf{high model performance on synthetic data does not guarantee real-world generalization}. We explicitly scope our claims to this benchmark and identify validation with production deployment data as critical future work.

\paragraph{Construct Validity---Critical Limitation} Recovery success is defined as state completeness $\geq$ 50\% and no fatal errors. This threshold is pragmatic but arbitrary; different thresholds would yield different success rates.

\textbf{Most importantly}: We measure \emph{system recovery} (script execution, file parsing: mean 29.4ms), \emph{not developer cognitive recovery} (the 15--25 minutes cited in prior work~\cite{mark2008cost, parnin2011programmer}). These are fundamentally different phenomena:
\begin{itemize}
    \item \textbf{System recovery}: Can the tool parse checkpoint files? (milliseconds)
    \item \textbf{Developer recovery}: Can the human regain mental context? (minutes)
\end{itemize}
We make no claim that predicting system recovery time (our contribution) implies anything about developer cognitive recovery (which requires user studies). Our practical value is predicting \textit{when systems will fail}---not \textit{how fast developers will re-engage}.

\paragraph{Corruption Model Simplicity} Our corruption simulation uses byte-level random corruption. Real-world failures may exhibit different patterns (e.g., incomplete writes, sector-level failures). More sophisticated fault injection models are future work.

\paragraph{Conclusion Validity} We use appropriate statistical methods: 5-fold cross-validation, 95\% confidence intervals, and non-parametric correlation (Spearman). Sample size (3,000 entries) provides adequate statistical power.

% ============================================================================
% REPRODUCIBILITY
% ============================================================================
\subsection{Reproducibility}

All experiments are reproducible via our replication package:
\begin{itemize}
    \item \textbf{Dataset}: 3,000 entries in JSON and CSV formats
    \item \textbf{Scripts}: \texttt{predictive\_dataset\_generator.py}, \texttt{train\_predictive\_models.py}, \texttt{compute\_baselines.py}
    \item \textbf{Environment}: \texttt{requirements.txt} with pinned versions (scikit-learn 1.6.1, pandas 2.2.3)
    \item \textbf{Docker}: Containerized environment for consistent results
    \item \textbf{Benchmark runner}: Single command (\texttt{./tests/benchmarks/benchmark\_runner.sh}) reproduces all experiments
\end{itemize}

